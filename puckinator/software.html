<!DOCTYPE html>
<!--
	Directive by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
  <head>
    <link rel="icon" href="images/favicon.png" />
    <title>Puckinator</title>
    <meta charset="utf-8" />
    <meta
      name="viewport"
      content="width=device-width, initial-scale=1, user-scalable=no"
    />
    <link rel="stylesheet" href="assets/css/main.css" />
    <script
      type="text/javascript"
      async
      src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML"
    ></script>
    <title>MathJax Equations</title>
  </head>
  <body>
    <nav id="top-menu">
      <ul>
        <li><a href="./index.html">Home</a></li>
        <li><a href="./design_decisions.html">Design Decisions</a></li>
        <li><a href="./electrical.html">Electrical</a></li>
        <li><a href="./mechanical.html">Mechanical</a></li>
        <li><a href="./firmware.html">Firmware</a></li>
        <li><a class="active" href="./software.html">Software</a></li>
        <li><a href="./photos.html">Photos and Videos</a></li>
        <li><a href="./budget.html">Budget</a></li>
      </ul>
    </nav>

    <!-- Sub Menu -->
    <div id="menu-container">
      <input type="checkbox" id="menu-toggle" />
      <label for="menu-toggle" class="menu-icon">&#9776; Menu</label>
      <!-- Customize as needed -->
      <nav id="sub-menu">
        <ul>
          <li><a href="#overview">Overview</a></li>
          <li><a href="#opencv">OpenCV Pipeline</a></li>
          <li><a href="#trajectory-prediction">Trajectory Prediction</a></li>
          <li><a href="#inverse-kinematics">Inverse Kinematics</a></li>
          <li><a href="#serial-comms">Serial Communications Interface</a></li>
          <li><a href="#external-dependencies">External Dependencies</a></li>
          <!-- Add more items as needed -->
        </ul>
      </nav>
    </div>

    <div class="box container">
      <header>
        <h2>Software Design</h2>
      </header>
      <section>
        <header>
          <p>
            Puckinator Software Overview:
            <a
              href="https://github.com/amandalchang/puckinator/blob/main/puckinator.py"
              >Source Code</a
            >
          </p>
        </header>
        <section id="overview">
          <hr />
          <h5>Overview</h5>
          <p>
            First, OpenCV transforms the frames from the camera to correct any
            distortion. The puck’s location is determined once this
            transformation is complete. The puck’s movement is tracked by
            predict_trajectory, which finds the optimal position for the striker
            to move to and the velocity vector of the puck. The stepper motor
            angles necessary for the striker to reach the position are
            calculated by coordinate_converter(). In the main loop, the optimal
            position and velocity vector are visualized on top of the OpenCV
            transformation and the angles are sent over serial to the firmware.
          </p>
        </section>
        <section id="opencv">
          <hr />
          <h5>OpenCV Pipeline</h5>
          <p>
            <b>Camera Capture:</b> <br />
            Our camera provides a 640x400 monochrome video stream in the MJPG
            format. We use the <code>imutils.video</code> module to put the
            OpenCV video stream on a separate thread. This allows us to avoid
            using the blocking <code>stream.read()</code> function on the main
            thread that also is responsible for trajectory prediction and
            sending commands to the Arduino.
          </p>
          <p>
            <b>Perspective Correction and Calibration:</b> <br />
            When the program starts, the camera view is approximately centered
            on the air hockey table, but there is some perspective distortion.
            To find the location of the puck with relation to the table, we need
            to perspective correct and crop the frame to the corners of the
            table. The corners are marked with ArUco markers and their positions
            are identified through the <code>cv.aruco</code>
            module. For each marker, we define a desired position within the
            transformed frame. Those positions correspond to the actual
            dimensions of the air hockey table (multiplied by the
            <code>PIXELS_PER_INCH</code> constant). The detected corners (from
            the ArUco markers) and desired corners are passed into the
            <code>cv.getPerspectiveTransform()</code> function which calculates
            the transformation matrix between the distorted webcam frame and the
            desired points.
          </p>
          <img
            src="images/software-images/PerspectiveTransform.svg"
            alt=""
            style="vertical-align: middle"
          />

          <p>
            On each new webcam frame, we call
            <code>cv.warpPerspective()</code> with the calculated transformation
            matrix to transform and crop the incoming image.
          </p>

          <p>
            <b>Puck Detection</b> <br />Once we have a perspective corrected
            frame, the next step is to detect the puck using the ArUco marker
            that’s taped onto it. We use the same
            <code>detectMarkers()</code> function from the
            <code>cv.aruco</code> module. Using those results, we draw the
            detected markers on the frame for visualization purposes. To find
            the puck, we filter the results to only look for a marker with the
            ID 4 (which is the ID of the marker taped to the puck). If there are
            any such results, we then find the center point of the puck by
            calculating the average position of all four corners. The data is
            returned in an instance of the
            <code>TimestampedPosition</code> dataclass which stores the position
            and timestamp when the marker detection completed.
          </p>
        </section>
        <section id="trajectory-prediction">
          <hr />
          <h5>Trajectory Prediction</h5>
          <p>
            In our main loop, we track the puck’s coordinate and timestamp at
            every frame and store it in an instance of the dataclass
            <code>TimestampedPos</code> , which has member variables x, y, and
            timestamp. In a while loop, we continually create new instances of
            <code>TimestampedPos</code> at every new frame, making sure to store
            one past instance of <code>TimestampedPos</code> each time.
          </p>
          <b>predict_trajectory():</b>
          <ol>
            <li>
              <p>
                The function <code>predict_trajectory</code> then receives a
                <code>previous_position</code>, a <code>latest_position</code>,
                and a constant <code>x_intercept</code> to calculate the
                velocity vector of the puck using the change in time and change
                in position. <code>HITTING_POSITION</code> represents the
                arbitrarily set x-value for which the robot will attempt to meet
                the puck. It also calculates the line that the velocity vector
                falls on, hereafter referenced as the velocity line. Once this
                vector is defined, there are three modes for the robot’s
                behavior: copying, puck not bouncing, and puck bouncing. These
                all output <code>y_int</code>, which is the y-coordinate of the
                desired position of the striker.
              </p>
              <ol>
                <li>
                  <p>
                    If the puck is headed away from the robot (i.e. change in
                    position is positive), has a speed less than a specified
                    <code>SPEED_THRESHOLD</code>, or is traveling parallel to
                    the y-axis, it simply copies the y-position of the puck and
                    outputs no velocity vector. If none of the listed conditions
                    are fulfilled, it moves on to two more options as detailed
                    below.
                  </p>
                </li>
                <li>
                  <p>
                    If the intersection between the velocity line and the
                    <code>HITTING_POSITION</code> is within the table’s bounds
                    (in other words, it will not bounce before the robot tries
                    to hit it), the function returns the y-value of the
                    intersection.
                  </p>
                </li>
                <li>
                  <p>
                    If it is projected to hit the side of the table before
                    reaching <code>HITTING_POSITION</code>:
                    <code>predict_next_bounce()</code> takes in the slope and
                    y-intercept of the velocity line. It then outputs a
                    predicted velocity line for after the bounce as well as the
                    point at which the puck should bounce. This is done using
                    the law of reflection, or the concept that the angle of
                    incidence will equal the angle of reflection. Since we have
                    lines rather than angles, we did this by negating the slope
                    and calculating the new y-intercept using simple point-slope
                    form. We determined which of the two walls (we only
                    calculated bounces off of the long sides of the table) that
                    the puck would hit based off the sign of the slope. We then
                    stored the velocity lines at each bounce point in a list,
                    which was outputted by the outer function
                    <code>predict_trajectory()</code>.
                  </p>
                </li>
              </ol>
            </li>
          </ol>
          <p>
            Aside from the velocity lines list, the function also outputs an
            instance of the dataclass <code>TrajectoryPrediction</code>, which
            includes the predicted x-position of the puck, the predicted
            y-position, the change in x, the change in y, and the predicted time
            that the puck will arrive at predicted position.
          </p>
          <b>Limitations of <code>predict_trajectory():</code></b>
          <p>
            Interestingly, the real world behavior of the puck did not perfectly
            match the law of reflection, possibly due to uneven air distribution
            from the table, the table being tilted, and friction on the surface,
            which contributed to jitteriness in the outputted y. To reduce this
            jitteriness, we manually limited number of bounces that the function
            would predict to 2.
          </p>
          <b>Visualizations:</b>
          <p>
            The velocity vector is displayed when the puck is not predicted to
            hit the side of the table before reaching the robot. If the puck
            bounces, it instead displays all of the velocity lines with
            directional arrows as limited by the edges of the table. A red
            circle is displayed around the point that the striker should move to
            to meet the puck. These are all drawn on top of the perspective
            transform that appears once the transformation is initiated with the
            “c” key. All of the visualization code lives outside
            <code>predict_trajectory()</code> in the main function.
          </p>
        </section>
        <section id="inverse-kinematics">
          <hr />
          <h5>Inverse Kinematics</h5>
          <img
            src="images/software-images/inverse_kinematics.svg"
            alt=""
            style="vertical-align: middle"
          />
          <p>
            Pictured above is an abstracted diagram of our five bar linkage.
            Point P represents the center of the striker, while sides A through
            D represent the arms of the linkage. (0, 0) in the inverse
            kinematics coordinate frame is the center of the left stepper motor
            shaft, while (0, 0) in the OpenCV coordinate frame is the upper left
            corner of the upper left ArUco Marker affixed to the table. Angles
            theta and phi represent the required angles of our two stepper
            motors to move the striker to point P.
          </p>
          <p>
            <b>Coordinate Conversion:</b> <br />In the file
            <code>puckinator.py</code>, a separate function
            <code>predict_trajectory()</code>
            determines the desired OpenCV y-position of the striker based on the
            behavior of the puck. The OpenCV x-position of the striker is set as
            a constant at all times, meaning the striker should only move side
            to side along the defined x-position.
          </p>

          <p>
            The function <code>coordinate_converter()</code> takes in a point
            <em>in the inverse kinematics coordinate frame</em> and outputs the
            angles theta and phi required to move the striker to that point. In
            every other function we only use the OpenCV coordinate frame. To
            convert from the OpenCV coordinate found by predict
            <code>trajectory()</code> to the inverse kinematics coordinate
            frame, we add
            <code>Y_OFFSET</code>
            to the inverse kinematics y-value and feed it into the coordinate
            converter as the x-position, because the x and y axes are switched.
            Similarly, we add <code>X_OFFSET</code> to the desired inverse
            kinematics x-value and feed it as the y-position.
          </p>
          <p>
            <b>Rectangular to Angular:</b> <br /><em>Theta:</em>
            \[\text{diag1} = \sqrt{x^2 + x^y}\]
            <br />
            The bottom angle of isosceles triangle, which I will call angle b,
            is defined by sides A, B and diag1. Realizing that A and B are both
            length L, the law of cosines therefore states: \[L^2 = L^2 +
            \text{diag1}^2 - 2L\text{diag1} \cos(b)\] \[b =
            \cos^{-1}\left(\frac{\text{diag1}}{2L}\right)\] From this, we
            conclude that: \[\theta = \tan^{-1}\left(\frac{y}{x}\right) +
            \cos^{-1}\left(\frac{\text{diag1}}{2L}\right)\]
            <br />

            <br /><em>Phi:</em>
            \[\text{diag2} = \sqrt{(s - x)^2 + y^2}\]
            <br />
            Angle c of the isosceles triangle defined by C, D, and diag2 can
            also be defined by law of cosines: \[c =
            \cos^{-1}\left(\frac{\text{diag2}}{2L}\right)\] <br />From this it
            follows: \[\phi = \pi -
            \left(\cos^{-1}\left(\frac{\text{diag2}}{2L}\right) +
            \tan^{-1}\left(\frac{y}{s - x}\right)\right)\] <br />
            These equations apply for when x > 0. However in the code
            implementation, we used the numpy function <code>arctan2()</code> so
            that the angle contributions of the two triangles defined by x, y
            and diag1 and by x, y, and diag 2 respectively would continue to
            function when point P had a negative x value. When the x value of
            the striker becomes negative, the right triangles mentioned
            previously flip vertically. For this reason, the arctangent
            calculation must be subtracted by 90 to get the new desired angle
            contribution. <code>arctan2()</code> does this automatically, hence
            its inclusion in the arctangent calculations within
            <code>coordinate_converter()</code>. For cases in which x = 0, we
            simply set x = 0.001 to avoid zero division errors.
          </p>
        </section>
        <section id="serial-comms">
          <hr />
          <h5>Serial Communications Interface</h5>
          <p>
            Once <code>predict_trajectory()</code> and
            <code>coordinate_converter()</code> have done their jobs finding the
            best theta and phi values for the arm to move to based on the puck’s
            behavior, we must send these values over serial to our Arduino,
            which is responsible for turning those values into real-world motor
            movement. Once we initialize the arduino with a high baud rate to
            reduce latency and a write timeout to protect against overloading
            the queue, this is as simple as using the serial method
            <code>write()</code> to transfer those theta and phi values over.
            The inputs are adjusted by 90 degrees because the stepper motors are
            zeroed at 90 degrees (As explained in the Firmware section).
          </p>
        </section>
        <section id="external-dependencies">
          <hr />
          <h5>External Dependencies</h5>
          <p>
            Our external python dependecies are listed in the requirements.txt
            file found on our
            <a href="https://github.com/amandalchang/puckinator">github</a>.
            They include OpenCV, NumPy, pySerial, time, math, datclasses,
            imutils, and the Arduino library FlexyStepper.
          </p>
        </section>
      </section>
    </div>
  </body>
</html>
